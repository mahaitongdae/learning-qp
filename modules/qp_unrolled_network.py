import torch
from torch import nn
from modules.qp_solver import QPSolver
from modules.warm_starter import WarmStarter
from utils.utils import make_psd, interpolate_state_dicts

class QPUnrolledNetwork(nn.Module):
    """
    Learn a QP problem from the input using a MLP, then solve the QP using fixed number of unrolled PDHG iterations.

    Form of QP:
    minimize    (1/2)x'Px + q'x
    subject to  Hx + b >= 0,
    where x in R^n, b in R^m.
    """
    def __init__(
        self, device, input_size, n_qp, m_qp, qp_iter, mlp_builder,
        shared_PH=False,
        use_warm_starter=False,
        train_warm_starter=False,
        ws_loss_coef=1.,
        ws_update_rate=0.01,
    ):
        """mlp_builder is a function mapping (input_size, output_size) to a nn.Sequential object.
        
        If shared_PH == True, P and H are parameters indepedent of input, and q and b are functions of input;
        Otherwise, (P, H, q, b) are all functions of input.
        """

        super().__init__()

        self.shared_PH = shared_PH
        self.device = device
        self.input_size = input_size
        self.n_qp = n_qp
        self.m_qp = m_qp
        self.qp_iter = qp_iter

        self.n_P_param = n_qp * (n_qp + 1) // 2
        self.n_q_param = n_qp
        self.n_H_param = m_qp * n_qp
        self.n_b_param = m_qp

        if self.shared_PH:
            self.n_mlp_output = self.n_q_param + self.n_b_param
            self.P_params = nn.Parameter(torch.randn((self.n_P_param,), device=device)) 
            self.H_params = nn.Parameter(torch.randn((self.n_H_param,), device=device)) 
        else:
            self.n_mlp_output = self.n_P_param + self.n_q_param + self.n_H_param + self.n_b_param

        self.mlp = mlp_builder(input_size, self.n_mlp_output)
        # TODO: add preconditioner
        self.warm_starter = WarmStarter(device, n_qp, m_qp, fixed_P=shared_PH, fixed_H=shared_PH) if use_warm_starter else None
        self.warm_starter_delayed = WarmStarter(device, n_qp, m_qp, fixed_P=shared_PH, fixed_H=shared_PH) if use_warm_starter else None
        self.train_warm_starter = train_warm_starter
        self.ws_loss_coef = ws_loss_coef
        self.ws_update_rate = ws_update_rate

        # is_warm_starter_trainable is always False, since the warm starter is trained via another inference independent of the solver
        self.solver = QPSolver(device, n_qp, m_qp, warm_starter=self.warm_starter_delayed, is_warm_starter_trainable=False)

        # Includes losses generated by the model itself (indepedent of interaction with env), i.e., warm starting & preconditioning
        self.autonomous_losses = {}

    def compute_warm_starter_loss(self, q, b, P, H, solver_Xs):
        qd, bd, Pd, Hd = map(lambda t: t.detach() if t is not None else None, [q, b, P, H])
        X0 = self.warm_starter(qd, bd, Pd, Hd)
        gt = solver_Xs[:, -1, :].detach()
        return self.ws_loss_coef * ((gt - X0) ** 2).sum(dim=-1).mean()

    def forward(self, x):
        bs = x.shape[0]
        qp_params = self.mlp(x)

        if not self.shared_PH:
            start = 0
            end = self.n_P_param
            P_params = qp_params[:, start:end]
            start = end
            end = start + self.n_q_param
            q = qp_params[:, start:end]
            start = end
            end = start + self.n_H_param
            H_params = qp_params[:, start:end]
            start = end
            end = start + self.n_b_param
            b = qp_params[:, start:end]
        else:
            start = 0
            end = self.n_q_param
            q = qp_params[:, start:end]
            start = end
            end = start + self.n_b_param
            b = qp_params[:, start:end]
            P_params = self.P_params.unsqueeze(0).broadcast_to((bs, -1))
            H_params = self.H_params.unsqueeze(0).broadcast_to((bs, -1))

        # Reshape P, H vectors into matrices
        P = make_psd(P_params, min_eig=1e-2)
        H = H_params.view(-1, self.m_qp, self.n_qp)

        # Update parameters of warm starter with a delay to stabilize training
        if self.train_warm_starter:
            self.warm_starter_delayed.load_state_dict(interpolate_state_dicts(self.warm_starter_delayed.state_dict(), self.warm_starter.state_dict(), self.ws_update_rate))

        Xs, primal_sols = self.solver(q, b, P, H, iters=self.qp_iter)
        if self.train_warm_starter:
            self.autonomous_losses["warm_starter"] = self.compute_warm_starter_loss(q, b, P, H, Xs)
        return primal_sols[:, -1, :]
